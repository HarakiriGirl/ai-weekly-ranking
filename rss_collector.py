#!/usr/bin/env python3
"""
GitHub Actionså¯¾å¿œRSSåé›†ã‚·ã‚¹ãƒ†ãƒ 
æ¯æ—¥è‡ªå‹•å®Ÿè¡Œã§21ã‚µã‚¤ãƒˆã‹ã‚‰RSSåé›†ã—ã€JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
"""

import feedparser
import json
import os
from datetime import datetime, timedelta
import time
import hashlib

def get_rss_feeds():
    """19ã‚µã‚¤ãƒˆã®RSS URLä¸€è¦§"""
    return {
        # è¶…å¤§æ‰‹
        "TechCrunch": "https://techcrunch.com/feed/",
        "The Verge": "https://www.theverge.com/rss/index.xml",
        "Wired": "https://www.wired.com/feed/rss",
        "Engadget": "https://www.engadget.com/rss.xml",
        "ITmedia": "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml",
        
        # å¤§æ‰‹
        "ZDNet": "https://www.zdnet.com/news/rss.xml",
        "CNET Japan": "https://www.cnet.com/rss/news/",
        "æ—¥çµŒXTECH": "https://xtech.nikkei.com/rss/xtech-it.rdf",
        "Ars Technica": "https://feeds.arstechnica.com/arstechnica/index",
        "VentureBeat": "https://venturebeat.com/feed/",
        "Mashable": "https://mashable.com/feeds/rss/all",
        "Impress Watch": "https://www.watch.impress.co.jp/data/rss/1.0/ipw/feed.rdf",
        "ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹": "https://news.mynavi.jp/rss/index",
        
        # ä¸­å …
        "MIT Technology Review": "https://www.technologyreview.com/feed/",
        "ASCII.jp": "https://ascii.jp/rss.xml",

        "ReadWrite": "https://readwrite.com/feed/",
        "The Next Web": "https://thenextweb.com/feed",
        
        # å°‚é–€ãƒ»ä¸­å°
        "Gigazine": "https://gigazine.net/news/rss_2.0/",
        "Publickey": "https://www.publickey1.jp/atom.xml",
        
        # è¿½åŠ ã‚µã‚¤ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ï¼‰
        # "PC Watch": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf",
        # "ã‚±ãƒ¼ã‚¿ã‚¤Watch": "https://k-tai.watch.impress.co.jp/data/rss/1.0/ktw/feed.rdf",
        # "INTERNET Watch": "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
        
        # é™¤å¤–ã‚µã‚¤ãƒˆï¼ˆã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ç­‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿï¼‰
        # "The Next Web": "https://thenextweb.com/feed",  # ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã«ã‚ˆã‚Šã‚¨ãƒ©ãƒ¼
    }

def clean_text(text):
    """HTMLã‚¿ã‚°ã‚„ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»"""
    import re
    if not text:
        return ""
    
    # HTMLã‚¿ã‚°é™¤å»
    text = re.sub(r'<[^>]*>', '', str(text))
    # ä½™åˆ†ãªç©ºç™½é™¤å»
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def collect_daily_rss():
    """å½“æ—¥ã®RSSè¨˜äº‹ã‚’åé›†"""
    rss_feeds = get_rss_feeds()
    today = datetime.now().strftime('%Y-%m-%d')
    
    result = {
        "collection_date": today,
        "total_sites": len(rss_feeds),
        "sites": {}
    }
    
    successful_sites = 0
    total_articles = 0
    
    print(f"ğŸ“¡ RSSåé›†é–‹å§‹: {today}")
    print(f"ğŸ“Š å¯¾è±¡ã‚µã‚¤ãƒˆ: {len(rss_feeds)}ã‚µã‚¤ãƒˆ")
    print("-" * 50)
    
    for site_name, rss_url in rss_feeds.items():
        print(f"ğŸ”„ å‡¦ç†ä¸­: {site_name}")
        
        try:
            # RSSå–å¾—
            feed = feedparser.parse(rss_url)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if feed.bozo:
                print(f"  âš ï¸  è­¦å‘Š: RSSè§£æã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰")
            
            articles = []
            
            # å„è¨˜äº‹ã‚’å‡¦ç†
            for entry in feed.entries:
                try:
                    # è¨˜äº‹æƒ…å ±æŠ½å‡º
                    article = {
                        "title": clean_text(entry.get('title', '')),
                        "summary": clean_text(entry.get('summary', ''))[:500],  # 500æ–‡å­—ã¾ã§
                        "link": entry.get('link', ''),
                        "published": entry.get('published', ''),
                    }
                    
                    # è¨˜äº‹IDç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
                    article_id = hashlib.md5(
                        (article['title'] + article['link']).encode('utf-8')
                    ).hexdigest()[:8]
                    
                    article['id'] = article_id
                    
                    # ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    if article['title']:
                        articles.append(article)
                
                except Exception as e:
                    print(f"    âŒ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            result["sites"][site_name] = {
                "url": rss_url,
                "articles_count": len(articles),
                "articles": articles,
                "status": "success"
            }
            
            successful_sites += 1
            total_articles += len(articles)
            print(f"  âœ… å®Œäº†: {len(articles)}ä»¶å–å¾—")
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            time.sleep(1)
            
        except Exception as e:
            result["sites"][site_name] = {
                "url": rss_url,
                "articles_count": 0,
                "articles": [],
                "status": "error",
                "error": str(e)
            }
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±è¿½åŠ 
    result["summary"] = {
        "successful_sites": successful_sites,
        "failed_sites": len(rss_feeds) - successful_sites,
        "total_articles": total_articles
    }
    
    print("-" * 50)
    print(f"ğŸ“ˆ åé›†å®Œäº†ã‚µãƒãƒªãƒ¼:")
    print(f"  æˆåŠŸ: {successful_sites}/{len(rss_feeds)}ã‚µã‚¤ãƒˆ")
    print(f"  ç·è¨˜äº‹æ•°: {total_articles}ä»¶")
    
    return result

def save_daily_data(data):
    """å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    today = datetime.now().strftime('%Y%m%d')
    filename = f"{data_dir}/rss_{today}.json"
    
    # JSONä¿å­˜
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filename}")
    return filename

def create_weekly_summary():
    """éå»7æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆé€±æœ«å®Ÿè¡Œç”¨ï¼‰"""
    data_dir = "data"
    
    # éå»7æ—¥åˆ†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    weekly_data = {
        "week_start": (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d'),
        "week_end": datetime.now().strftime('%Y-%m-%d'),
        "daily_files": [],
        "all_articles": [],
        "site_summary": {}
    }
    
    for i in range(7):
        date = datetime.now() - timedelta(days=i)
        filename = f"{data_dir}/rss_{date.strftime('%Y%m%d')}.json"
        
        if os.path.exists(filename):
            print(f"ğŸ“ èª­ã¿è¾¼ã¿: {filename}")
            
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    daily_data = json.load(f)
                
                weekly_data["daily_files"].append({
                    "date": date.strftime('%Y-%m-%d'),
                    "filename": filename,
                    "articles_count": daily_data.get("summary", {}).get("total_articles", 0)
                })
                
                # å…¨è¨˜äº‹ã‚’çµ±åˆ
                for site_name, site_data in daily_data.get("sites", {}).items():
                    if site_data.get("status") == "success":
                        for article in site_data.get("articles", []):
                            article["site"] = site_name
                            article["collection_date"] = date.strftime('%Y-%m-%d')
                            weekly_data["all_articles"].append(article)
                
            except Exception as e:
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filename} - {str(e)}")
    
    # é‡è¤‡è¨˜äº‹é™¤å»
    unique_articles = {}
    for article in weekly_data["all_articles"]:
        article_id = article.get("id")
        if article_id and article_id not in unique_articles:
            unique_articles[article_id] = article
    
    weekly_data["all_articles"] = list(unique_articles.values())
    weekly_data["total_unique_articles"] = len(unique_articles)
    
    # ã‚µã‚¤ãƒˆåˆ¥çµ±è¨ˆ
    site_stats = {}
    for article in weekly_data["all_articles"]:
        site = article["site"]
        if site not in site_stats:
            site_stats[site] = 0
        site_stats[site] += 1
    
    weekly_data["site_summary"] = site_stats
    
    # é€±é–“ã‚µãƒãƒªãƒ¼ä¿å­˜
    week_filename = f"{data_dir}/weekly_summary_{datetime.now().strftime('%Y%m%d')}.json"
    with open(week_filename, 'w', encoding='utf-8') as f:
        json.dump(weekly_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š é€±é–“ã‚µãƒãƒªãƒ¼ä¿å­˜: {week_filename}")
    print(f"ğŸ“ˆ çµ±è¨ˆ: {len(weekly_data['daily_files'])}æ—¥åˆ†ã€{weekly_data['total_unique_articles']}ä»¶ï¼ˆé‡è¤‡é™¤å»å¾Œï¼‰")
    
    return week_filename

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # å¼•æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆGitHub Actionsã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ï¼‰
        import sys
        mode = sys.argv[1] if len(sys.argv) > 1 else "daily"
        
        if mode == "weekly":
            print("ğŸ—“ï¸  é€±é–“ã‚µãƒãƒªãƒ¼ä½œæˆãƒ¢ãƒ¼ãƒ‰")
            create_weekly_summary()
        else:
            print("ğŸ“… æ—¥æ¬¡RSSåé›†ãƒ¢ãƒ¼ãƒ‰")
            # æ—¥æ¬¡RSSåé›†
            data = collect_daily_rss()
            save_daily_data(data)
        
        print("âœ… å‡¦ç†å®Œäº†")
        
    except Exception as e:
        print(f"ğŸ’¥ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        exit(1)

if __name__ == "__main__":
    main()
