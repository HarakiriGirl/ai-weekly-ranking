#!/usr/bin/env python3
"""
GitHub Actionså¯¾å¿œRSSåé›†ã‚·ã‚¹ãƒ†ãƒ 
æ¯æ—¥è‡ªå‹•å®Ÿè¡Œã§22ã‚µã‚¤ãƒˆã‹ã‚‰RSSåé›†ã—ã€JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
"""

import feedparser
import json
import os
import re
from datetime import datetime, timedelta
import time
import hashlib

def get_rss_feeds():
    """22ã‚µã‚¤ãƒˆã®RSS URLä¸€è¦§"""
    return {
        # è¶…å¤§æ‰‹
        "TechCrunch": "https://techcrunch.com/feed/",
        "The Verge": "https://www.theverge.com/rss/index.xml",
        "Wired": "https://www.wired.com/feed/rss",
        "Engadget": "https://www.engadget.com/rss.xml",
        "ITmedia": "https://rss.itmedia.co.jp/rss/2.0/news_bursts.xml",
        
        # å¤§æ‰‹
        "ZDNet": "https://www.zdnet.com/news/rss.xml",
        "CNET Japan": "https://www.cnet.com/rss/news/",
        "æ—¥çµŒXTECH": "https://xtech.nikkei.com/rss/xtech-it.rdf",
        "Ars Technica": "https://feeds.arstechnica.com/arstechnica/index",
        "VentureBeat": "https://venturebeat.com/feed/",
        "Mashable": "https://mashable.com/feeds/rss/all",
        "Impress Watch": "https://www.watch.impress.co.jp/data/rss/1.0/ipw/feed.rdf",
        "ãƒã‚¤ãƒŠãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹": "https://news.mynavi.jp/rss/index",
        
        # ä¸­å …
        "MIT Technology Review": "https://www.technologyreview.com/feed/",
        "ASCII.jp": "https://ascii.jp/rss.xml",
        "ReadWrite": "https://readwrite.com/feed/",# ã‚¨ãƒ©ãƒ¼é »ç™ºãªã‚‰å‰Šé™¤
        "The Next Web": "https://thenextweb.com/feed",
        
        # å°‚é–€ãƒ»ä¸­å°
        "Gigazine": "https://gigazine.net/news/rss_2.0/",
        "Publickey": "https://www.publickey1.jp/atom.xml",

        # ã‚¢ã‚¸ã‚¢ãƒ»EU
        "TechEU": "https://tech.eu/feed/",
        "TechRadar": "https://www.techradar.com/rss",
        "Tech Advisor": "https://www.techadvisor.com/feed/",
        
        # è¿½åŠ ã‚µã‚¤ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆè§£é™¤ï¼‰
        # "PC Watch": "https://pc.watch.impress.co.jp/data/rss/1.0/pcw/feed.rdf",
        # "ã‚±ãƒ¼ã‚¿ã‚¤Watch": "https://k-tai.watch.impress.co.jp/data/rss/1.0/ktw/feed.rdf",
        # "INTERNET Watch": "https://internet.watch.impress.co.jp/data/rss/1.0/iw/feed.rdf",
        
        # é™¤å¤–ã‚µã‚¤ãƒˆï¼ˆã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ç­‰ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿï¼‰
        # "The Next Web": "https://thenextweb.com/feed",  # ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã«ã‚ˆã‚Šã‚¨ãƒ©ãƒ¼
    }

def clean_text(text):
    """HTMLã‚¿ã‚°ã‚„ä½™åˆ†ãªç©ºç™½ã‚’é™¤å»"""
    if not text:
        return ""
    
    # HTMLã‚¿ã‚°é™¤å»
    text = re.sub(r'<[^>]*>', '', str(text))
    # ä½™åˆ†ãªç©ºç™½é™¤å»
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def parse_published_date(published_str):
    """publishedæ–‡å­—åˆ—ã‚’æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    if not published_str:
        return None
    
    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãISOå½¢å¼ã®ç‰¹åˆ¥å‡¦ç†ï¼ˆThe Vergeç­‰ï¼‰
    if 'T' in published_str and ('+' in published_str or '-' in published_str[-6:]):
        try:
            # 2025-06-09T11:24:23-04:00 å½¢å¼
            # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³éƒ¨åˆ†ã‚’é™¤å»
            clean_str = re.sub(r'[+-]\d{2}:\d{2}$', '', published_str)
            return datetime.strptime(clean_str, '%Y-%m-%dT%H:%M:%S')
        except:
            pass
    
    # æ¨™æº–çš„ãªæ—¥ä»˜å½¢å¼ã‚’è©¦è¡Œ
    formats = [
        '%a, %d %b %Y %H:%M:%S %z',     # Mon, 09 Jun 2025 09:35:00 +0000
        '%a, %d %b %Y %H:%M:%S',        # Mon, 09 Jun 2025 09:35:00
        '%Y-%m-%dT%H:%M:%S%z',          # 2025-06-09T09:35:00+00:00
        '%Y-%m-%dT%H:%M:%S',            # 2025-06-09T09:35:00
        '%Y-%m-%d %H:%M:%S',            # 2025-06-09 09:35:00
        '%Y-%m-%d',                     # 2025-06-09
    ]
    
    for fmt in formats:
        try:
            # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ãŒã‚ã‚Œã°é™¤å»ã—ã¦å‡¦ç†
            clean_str = published_str.replace(' +0000', '').replace(' +0900', '').replace(' GMT', '').replace('Z', '')
            return datetime.strptime(clean_str, fmt.replace('%z', ''))
        except:
            continue
    
    # å…¨å½¢å¼ã§å¤±æ•—ã—ãŸå ´åˆã¯None
    return None

def collect_daily_rss():
    """å½“æ—¥ã®RSSè¨˜äº‹ã‚’åé›†"""
    rss_feeds = get_rss_feeds()
    today = datetime.now().strftime('%Y-%m-%d')
    
    result = {
        "collection_date": today,
        "total_sites": len(rss_feeds),
        "sites": {}
    }
    
    successful_sites = 0
    total_articles = 0
    
    print(f"ğŸ“¡ RSSåé›†é–‹å§‹: {today}")
    print(f"ğŸ“Š å¯¾è±¡ã‚µã‚¤ãƒˆ: {len(rss_feeds)}ã‚µã‚¤ãƒˆ")
    print("-" * 50)
    
    for site_name, rss_url in rss_feeds.items():
        print(f"ğŸ”„ å‡¦ç†ä¸­: {site_name}")
        
        try:
            # RSSå–å¾—
            feed = feedparser.parse(rss_url)
            
            # ã‚¨ãƒ©ãƒ¼ãƒã‚§ãƒƒã‚¯
            if feed.bozo:
                print(f"  âš ï¸  è­¦å‘Š: RSSè§£æã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰")
            
            articles = []
            
            # å„è¨˜äº‹ã‚’å‡¦ç†
            for entry in feed.entries:
                try:
                    # è¨˜äº‹æƒ…å ±æŠ½å‡º
                    article = {
                        "title": clean_text(entry.get('title', '')),
                        "summary": clean_text(entry.get('summary', ''))[:500],  # 500æ–‡å­—ã¾ã§
                        "link": entry.get('link', ''),
                        "published": entry.get('published', ''),
                    }
                    
                    # è¨˜äº‹IDç”Ÿæˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰- ã‚µã‚¤ãƒˆåã‚’å«ã‚ã‚‹
                    article_id = hashlib.md5(
                        (site_name + article['title'] + article['link']).encode('utf-8')
                    ).hexdigest()[:8]
                    
                    article['id'] = article_id
                    
                    # ç©ºã®ã‚¿ã‚¤ãƒˆãƒ«ã¯é™¤å¤–
                    if article['title']:
                        articles.append(article)
                
                except Exception as e:
                    print(f"    âŒ è¨˜äº‹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue
            
            result["sites"][site_name] = {
                "url": rss_url,
                "articles_count": len(articles),
                "articles": articles,
                "status": "success"
            }
            
            successful_sites += 1
            total_articles += len(articles)
            print(f"  âœ… å®Œäº†: {len(articles)}ä»¶å–å¾—")
            
            # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
            time.sleep(1)
            
        except Exception as e:
            result["sites"][site_name] = {
                "url": rss_url,
                "articles_count": 0,
                "articles": [],
                "status": "error",
                "error": str(e)
            }
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±è¿½åŠ 
    result["summary"] = {
        "successful_sites": successful_sites,
        "failed_sites": len(rss_feeds) - successful_sites,
        "total_articles": total_articles
    }
    
    print("-" * 50)
    print(f"ğŸ“ˆ åé›†å®Œäº†ã‚µãƒãƒªãƒ¼:")
    print(f"  æˆåŠŸ: {successful_sites}/{len(rss_feeds)}ã‚µã‚¤ãƒˆ")
    print(f"  ç·è¨˜äº‹æ•°: {total_articles}ä»¶")
    
    return result

def save_daily_data(data):
    """å½“æ—¥ã®ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
    today = datetime.now().strftime('%Y%m%d')
    filename = f"{data_dir}/rss_{today}.json"
    
    # JSONä¿å­˜
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filename}")
    return filename

def create_weekly_summary():
    """éå»7æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆï¼ˆé€±æœ«å®Ÿè¡Œç”¨ï¼‰"""
    data_dir = "data"
    
    # éå»7æ—¥åˆ†ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
    weekly_data = {
        "week_start": (datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d'),
        "week_end": datetime.now().strftime('%Y-%m-%d'),
        "daily_files": [],
        "all_articles": [],
        "site_summary": {}
    }
    
    daily_files_list = []  # ãƒ­ã‚°ç”¨ã«ä¿æŒ
    
    print(f"ğŸ—“ï¸  7æ—¥é–“ãƒ‡ãƒ¼ã‚¿çµ±åˆ: {weekly_data['week_start']} ï½ {weekly_data['week_end']}")
    
    for i in range(7):
        date = datetime.now() - timedelta(days=i)
        filename = f"{data_dir}/rss_{date.strftime('%Y%m%d')}.json"
        
        if os.path.exists(filename):
            print(f"ğŸ“ èª­ã¿è¾¼ã¿: {filename}")
            daily_files_list.append(filename)  # ãƒ­ã‚°ç”¨ãƒªã‚¹ãƒˆã«è¿½åŠ 
            
            try:
                with open(filename, 'r', encoding='utf-8') as f:
                    daily_data = json.load(f)
                
                # å…¨è¨˜äº‹ã‚’çµ±åˆ
                for site_name, site_data in daily_data.get("sites", {}).items():
                    if site_data.get("status") == "success":
                        for article in site_data.get("articles", []):
                            article["site"] = site_name
                            article["collection_date"] = date.strftime('%Y-%m-%d')
                            weekly_data["all_articles"].append(article)
                
            except Exception as e:
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {filename} - {str(e)}")
    
    print(f"ğŸ“Š çµ±åˆå‰è¨˜äº‹æ•°: {len(weekly_data['all_articles'])}ä»¶")
    
    # ğŸ“… æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆ7æ—¥ä»¥å†…ã®è¨˜äº‹ã®ã¿ä¿æŒï¼‰
    seven_days_ago = datetime.now() - timedelta(days=7)
    filtered_articles = []
    
    date_parse_success = 0
    date_parse_failed = 0
    filtered_out = 0
    
    print(f"ğŸ“… æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œï¼ˆåŸºæº–: {seven_days_ago.strftime('%Y-%m-%d %H:%M')}ï¼‰")
    
    for article in weekly_data["all_articles"]:
        published = article.get('published', '')
        parsed_date = parse_published_date(published)
        
        if parsed_date is None:
            # æ—¥ä»˜ä¸æ˜ã®è¨˜äº‹ã¯7æ—¥ä»¥å†…ã¨ã—ã¦æ‰±ã†ï¼ˆä¿æŒï¼‰
            filtered_articles.append(article)
            date_parse_failed += 1
        elif parsed_date >= seven_days_ago:
            # 7æ—¥ä»¥å†…ã®è¨˜äº‹ã¯ä¿æŒ
            filtered_articles.append(article)
            date_parse_success += 1
        else:
            # 7æ—¥ã‚ˆã‚Šå¤ã„è¨˜äº‹ã¯é™¤å¤–
            filtered_out += 1
    
    weekly_data["all_articles"] = filtered_articles
    
    print(f"ğŸ“ˆ æ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµæœ:")
    print(f"  æ—¥ä»˜è§£ææˆåŠŸ: {date_parse_success}ä»¶")
    print(f"  æ—¥ä»˜ä¸æ˜ï¼ˆä¿æŒï¼‰: {date_parse_failed}ä»¶")
    print(f"  7æ—¥ã‚ˆã‚Šå¤ã„ï¼ˆé™¤å¤–ï¼‰: {filtered_out}ä»¶")
    print(f"  ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_articles)}ä»¶")
    
    # é‡è¤‡è¨˜äº‹é™¤å»
    unique_articles = {}
    for article in weekly_data["all_articles"]:
        article_id = article.get("id")
        if article_id and article_id not in unique_articles:
            unique_articles[article_id] = article
    
    weekly_data["all_articles"] = list(unique_articles.values())
    weekly_data["total_unique_articles"] = len(unique_articles)
    
    print(f"ğŸ”„ é‡è¤‡é™¤å»å¾Œ: {len(unique_articles)}ä»¶")
    
    # ã‚µã‚¤ãƒˆåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆã‚¹ãƒªãƒ åŒ–ã•ã‚ŒãŸå‡ºåŠ›å½¢å¼ï¼‰
    sites_grouped = {}
    for article in weekly_data["all_articles"]:
        site = article["site"]
        if site not in sites_grouped:
            sites_grouped[site] = []
        
        # æ—¥ä»˜ã‚’ç°¡æ½”ãªå½¢å¼ã«å¤‰æ›
        published_date = ""
        if article.get("published"):
            parsed = parse_published_date(article["published"])
            if parsed:
                published_date = parsed.strftime("%Y-%m-%d")
            else:
                # æ—¥ä»˜è§£æå¤±æ•—æ™‚ã¯å…ƒã®æ–‡å­—åˆ—ã®æ—¥ä»˜éƒ¨åˆ†ã‚’æŠ½å‡º
                try:
                    if len(article["published"]) >= 10:
                        published_date = article["published"][:10]
                except:
                    published_date = ""
        
        # ã‚¹ãƒªãƒ åŒ–ã•ã‚ŒãŸè¨˜äº‹ãƒ‡ãƒ¼ã‚¿
        slim_article = {
            "title": article["title"],
            "summary": article["summary"],
            "published": published_date
        }
        sites_grouped[site].append(slim_article)
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°çµ±è¨ˆã‚’è¿½åŠ 
    filter_ratio = len(weekly_data["all_articles"]) / (len(weekly_data["all_articles"]) + filtered_out) * 100 if len(weekly_data["all_articles"]) + filtered_out > 0 else 0
    
    # o3å°‚ç”¨ã®ã‚¹ãƒªãƒ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã®ã¿ä½œæˆ
    final_output = {
        "week_start": weekly_data["week_start"],
        "week_end": weekly_data["week_end"],
        "total_articles": len(weekly_data["all_articles"]),
        "sites": sites_grouped
    }
    
    weekly_data = final_output
    
    # é€±é–“ã‚µãƒãƒªãƒ¼ä¿å­˜
    week_filename = f"{data_dir}/weekly_summary_{datetime.now().strftime('%Y%m%d')}.json"
    with open(week_filename, 'w', encoding='utf-8') as f:
        json.dump(weekly_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“Š é€±é–“ã‚µãƒãƒªãƒ¼ä¿å­˜: {week_filename}")
    print(f"ğŸ“ˆ æœ€çµ‚çµ±è¨ˆ: {len(daily_files_list)}æ—¥åˆ†ã€{weekly_data['total_articles']}ä»¶")
    print(f"ğŸ¯ ä¿æŒç‡: {filter_ratio:.1f}%")
    print(f"ğŸ—‚ï¸  ã‚µã‚¤ãƒˆåˆ¥ã‚°ãƒ«ãƒ¼ãƒ—: {len(sites_grouped)}ã‚µã‚¤ãƒˆ")
    
    return week_filename

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        # å¼•æ•°ãƒã‚§ãƒƒã‚¯ï¼ˆGitHub Actionsã‹ã‚‰æ¸¡ã•ã‚Œã‚‹ï¼‰
        import sys
        mode = sys.argv[1] if len(sys.argv) > 1 else "daily"
        
        if mode == "weekly":
            print("ğŸ—“ï¸  é€±é–“ã‚µãƒãƒªãƒ¼ä½œæˆãƒ¢ãƒ¼ãƒ‰")
            create_weekly_summary()
        else:
            print("ğŸ“… æ—¥æ¬¡RSSåé›†ãƒ¢ãƒ¼ãƒ‰")
            # æ—¥æ¬¡RSSåé›†
            data = collect_daily_rss()
            save_daily_data(data)
        
        print("âœ… å‡¦ç†å®Œäº†")
        
    except Exception as e:
        print(f"ğŸ’¥ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        exit(1)

if __name__ == "__main__":
    main()
