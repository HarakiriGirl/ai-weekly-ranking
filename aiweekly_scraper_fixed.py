#!/usr/bin/env python3
"""
AI-Weeklyå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
æ¯é€±ç«æ›œæ—¥ã«AI-weeklyã®æ–°è¨˜äº‹ã‚’å–å¾—
"""

import feedparser
import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import time

def get_aiweekly_rss():
    """AI-Weeklyã®RSS URL"""
    return "https://ai-weekly.ai/feed/"

def fetch_new_articles():
    """AI-Weeklyã®RSSã‹ã‚‰æ–°è¨˜äº‹URLã‚’å–å¾—"""
    print("ğŸ“¡ AI-Weekly RSSå–å¾—é–‹å§‹")
    
    try:
        # RSSå–å¾—
        feed = feedparser.parse(get_aiweekly_rss())
        
        if feed.bozo:
            print(f"âš ï¸  RSSè§£æè­¦å‘Š: {feed.bozo_exception}")
        
        articles = []
        print(f"ğŸ“° RSSè¨˜äº‹æ•°: {len(feed.entries)}ä»¶")
        
        # å„è¨˜äº‹ã®URLã‚’åé›†
        for entry in feed.entries[:1]:  # æœ€æ–°1ä»¶ã®ã¿å‡¦ç†
            article_info = {
                "title": entry.get('title', ''),
                "link": entry.get('link', ''),
                "published": entry.get('published', ''),
                "description": entry.get('description', '')
            }
            
            if article_info['link']:
                articles.append(article_info)
                print(f"  ğŸ“„ {article_info['title']}")
        
        return articles
        
    except Exception as e:
        print(f"âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def scrape_article_content(url):
    """å€‹åˆ¥è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ¬æ–‡ã‚’å–å¾—"""
    print(f"ğŸ” ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")
    
    try:
        # User-Agentã‚’è¨­å®šã—ã¦ã‚¢ã‚¯ã‚»ã‚¹
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        # BeautifulSoupã§è§£æ
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # WordPressã®è¨˜äº‹ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ¢ã™
        content_selectors = [
            '.entry-content',           # ä¸€èˆ¬çš„ãªWordPressãƒ†ãƒ¼ãƒ
            '.post-content',            # åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³
            'article .content',         # articleå†…ã®content
            '.content',                 # ã‚·ãƒ³ãƒ—ãƒ«ãªcontent
            'main',                     # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        ]
        
        content_text = ""
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ãƒ†ã‚­ã‚¹ãƒˆã®ã¿æŠ½å‡ºï¼ˆHTMLã‚¿ã‚°é™¤å»ï¼‰
                content_text = content_elem.get_text(separator='\n', strip=True)
                print(f"  âœ… ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å–å¾—æˆåŠŸ: {len(content_text)}æ–‡å­—")
                break
        
        if not content_text:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: bodyå…¨ä½“ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
            body = soup.find('body')
            if body:
                content_text = body.get_text(separator='\n', strip=True)
                print(f"  âš ï¸  ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å–å¾—: {len(content_text)}æ–‡å­—")
        
        # ç©ºç™½è¡Œã®æ•´ç†
        lines = [line.strip() for line in content_text.split('\n') if line.strip()]
        content_text = '\n'.join(lines)
        
        return {
            "url": url,
            "content": content_text,
            "content_length": len(content_text),
            "scraped_at": datetime.now().isoformat(),
            "status": "success"
        }
        
    except requests.RequestException as e:
        print(f"  âŒ HTTP ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"url": url, "content": "", "status": "http_error", "error": str(e)}
    
    except Exception as e:
        print(f"  âŒ è§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {"url": url, "content": "", "status": "parse_error", "error": str(e)}

def process_aiweekly_articles():
    """AI-Weeklyã®è¨˜äº‹ã‚’å‡¦ç†ã—ã¦JSONã«ä¿å­˜"""
    today = datetime.now().strftime('%Y-%m-%d')
    
    result = {
        "collection_date": today,
        "source": "AI-Weekly",
        "rss_url": get_aiweekly_rss(),
        "articles": []
    }
    
    print(f"ğŸ¤– AI-Weeklyè¨˜äº‹åé›†é–‹å§‹: {today}")
    print("-" * 50)
    
    # RSSè¨˜äº‹ä¸€è¦§å–å¾—
    rss_articles = fetch_new_articles()
    
    if not rss_articles:
        print("âŒ RSSè¨˜äº‹ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return result
    
    # å„è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    for i, article_info in enumerate(rss_articles, 1):
        print(f"\nğŸ“– è¨˜äº‹ {i}/{len(rss_articles)}")
        
        # æœ¬æ–‡å–å¾—
        scraped_content = scrape_article_content(article_info['link'])
        
        # è¨˜äº‹ãƒ‡ãƒ¼ã‚¿çµ±åˆ
        article_data = {
            "title": article_info['title'],
            "link": article_info['link'],
            "published": article_info['published'],
            "description": article_info['description'],
            "content": scraped_content['content'],
            "content_length": scraped_content['content_length'],
            "scraping_status": scraped_content['status']
        }
        
        if scraped_content['status'] != 'success':
            article_data['error'] = scraped_content.get('error', '')
        
        result['articles'].append(article_data)
        
        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›
        time.sleep(2)
    
    # çµ±è¨ˆæƒ…å ±
    successful_scrapes = sum(1 for a in result['articles'] if a['scraping_status'] == 'success')
    total_content_length = sum(a['content_length'] for a in result['articles'])
    
    result['summary'] = {
        "total_articles": len(result['articles']),
        "successful_scrapes": successful_scrapes,
        "failed_scrapes": len(result['articles']) - successful_scrapes,
        "total_content_length": total_content_length
    }
    
    print("-" * 50)
    print(f"ğŸ“Š å‡¦ç†å®Œäº†:")
    print(f"  è¨˜äº‹æ•°: {len(result['articles'])}ä»¶")
    print(f"  æˆåŠŸ: {successful_scrapes}ä»¶")
    print(f"  å¤±æ•—: {len(result['articles']) - successful_scrapes}ä»¶")
    print(f"  ç·æ–‡å­—æ•°: {total_content_length:,}æ–‡å­—")
    
    return result

def save_aiweekly_data(data):
    """AI-Weeklyã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
    # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    data_dir = "data"
    os.makedirs(data_dir, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆJSTåŸºæº–ï¼‰
    jst_time = datetime.utcnow() + timedelta(hours=9)
    today = jst_time.strftime('%Y%m%d')
    filename = f"{data_dir}/aiweekly_{today}.json"
    
    # JSONä¿å­˜
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filename}")
    return filename

def test_single_article():
    """ãƒ†ã‚¹ãƒˆç”¨: 1è¨˜äº‹ã®ã¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
    test_url = "https://ai-weekly.ai/newsletter-06-10-2025/"
    print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ: {test_url}")
    
    result = scrape_article_content(test_url)
    
    print(f"\nğŸ“Š ãƒ†ã‚¹ãƒˆçµæœ:")
    print(f"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result['status']}")
    print(f"  æ–‡å­—æ•°: {result['content_length']:,}æ–‡å­—")
    
    if result['content']:
        # æœ€åˆã®500æ–‡å­—ã‚’è¡¨ç¤º
        preview = result['content'][:500] + "..." if len(result['content']) > 500 else result['content']
        print(f"\nğŸ“„ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼:")
        print(preview)
    
    return result

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    try:
        import sys
        
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰
        if len(sys.argv) > 1 and sys.argv[1] == "test":
            print("ğŸ§ª ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ")
            test_single_article()
            return
        
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰
        print("ğŸ¤– AI-Weeklyåé›†ãƒ¢ãƒ¼ãƒ‰")
        data = process_aiweekly_articles()
        save_aiweekly_data(data)
        
        print("âœ… å‡¦ç†å®Œäº†")
        
    except Exception as e:
        print(f"ğŸ’¥ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        exit(1)

if __name__ == "__main__":
    main()
