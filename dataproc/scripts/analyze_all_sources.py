#!/usr/bin/env python3
"""
å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†æï¼šRSSã€YouTubeã€AI-Weekly ã®è²¢çŒ®åº¦æ¯”è¼ƒ
"""

import json
import pandas as pd
from pathlib import Path
from collections import Counter
import re
from datetime import datetime, timedelta

def load_rss_data():
    """RSSãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    weekly_file = Path("data/rss/weekly/weekly_summary_20250613.json")
    
    if not weekly_file.exists():
        return None, "RSSé€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã—"
    
    with open(weekly_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    articles = []
    for site_name, site_articles in data['sites'].items():
        for article in site_articles:
            articles.append({
                'source_type': 'RSS',
                'source_name': site_name,
                'title': article['title'],
                'summary': article.get('summary', ''),
                'published': article.get('published', '')
            })
    
    return articles, f"RSS: {len(articles)}ä»¶"

def load_youtube_data():
    """YouTubeãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    youtube_dir = Path("data/youtube/weekly")
    youtube_files = list(youtube_dir.glob("youtube_weekly_*.json"))
    
    if not youtube_files:
        return None, "YouTubeé€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã—"
    
    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
    latest_file = max(youtube_files, key=lambda x: x.stat().st_mtime)
    print(f"  èª­ã¿è¾¼ã¿: {latest_file.name}")
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"  ãƒ‡ãƒ¼ã‚¿æ§‹é€ ç¢ºèª: {list(data.keys())}")
        
        articles = []
        for video in data.get('videos', []):
            articles.append({
                'source_type': 'YouTube',
                'source_name': video.get('channel_title', 'Unknown'),
                'title': video.get('title', ''),
                'summary': video.get('description', '')[:500],  # æœ€åˆã®500æ–‡å­—
                'published': video.get('published_at', '')
            })
        
        print(f"  å‡¦ç†å¾Œè¨˜äº‹æ•°: {len(articles)}ä»¶")
        return articles, f"YouTube: {len(articles)}ä»¶"
    
    except Exception as e:
        print(f"  YouTubeãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return None, f"YouTubeå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"

def load_aiweekly_data():
    """AI-Weeklyãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    aiweekly_dir = Path("data/aiweekly/weekly")
    aiweekly_files = list(aiweekly_dir.glob("aiweekly_*.json"))
    
    if not aiweekly_files:
        return None, "AI-Weeklyé€±æ¬¡ãƒ‡ãƒ¼ã‚¿ãªã—"
    
    # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
    latest_file = max(aiweekly_files, key=lambda x: x.stat().st_mtime)
    print(f"  èª­ã¿è¾¼ã¿: {latest_file.name}")
    
    with open(latest_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    articles = []
    
    # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã”ã¨ã«è¨˜äº‹ã‚’æŠ½å‡º
    for section_name, section_data in data.get('sections', {}).items():
        if isinstance(section_data, list):
            for item in section_data:
                if isinstance(item, dict):
                    articles.append({
                        'source_type': 'AI-Weekly',
                        'source_name': f"AI-Weekly/{section_name}",
                        'title': item.get('title', ''),
                        'summary': item.get('description', ''),
                        'published': data.get('date', '')
                    })
    
    return articles, f"AI-Weekly: {len(articles)}ä»¶"

def detect_ai_keywords(text):
    """AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œå‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    if not text:
        return []
    
    text_lower = text.lower()
    
    # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸
    ai_keywords = {
        'ai_general': ['ai', 'artificial intelligence', 'machine intelligence'],
        'ml_dl': ['machine learning', 'ml', 'deep learning', 'neural network', 'neural', 'cnn', 'rnn', 'lstm'],
        'models': ['gpt', 'llm', 'bert', 'transformer', 'diffusion', 'gan', 'vae'],
        'companies': ['openai', 'anthropic', 'meta ai', 'google ai', 'mistral', 'claude', 'chatgpt'],
        'applications': ['copilot', 'agent', 'automation', 'reasoning', 'computer vision', 'nlp'],
        'tools': ['stable diffusion', 'midjourney', 'dall-e', 'whisper', 'codex'],
        'techniques': ['fine-tuning', 'rag', 'prompt engineering', 'few-shot', 'zero-shot'],
        'models_specific': ['o1', 'o3', 'claude-3', 'gemini', 'palm', 'llama']
    }
    
    detected = []
    for category, keywords in ai_keywords.items():
        for keyword in keywords:
            if keyword in text_lower:
                detected.append((category, keyword))
    
    return detected

def analyze_source_data(articles, source_type):
    """ã‚½ãƒ¼ã‚¹åˆ¥ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
    if not articles:
        return None
    
    print(f"\n{'='*20} {source_type} åˆ†æ {'='*20}")
    print(f"ç·è¨˜äº‹æ•°: {len(articles)}ä»¶")
    
    # ã‚½ãƒ¼ã‚¹ååˆ¥çµ±è¨ˆ
    source_counts = Counter(article['source_name'] for article in articles)
    print(f"\nğŸ“Š {source_type}å†…è¨³:")
    for source_name, count in source_counts.most_common(10):
        print(f"  {source_name}: {count}ä»¶")
    if len(source_counts) > 10:
        print(f"  ... ä»–{len(source_counts)-10}ã‚½ãƒ¼ã‚¹")
    
    # AIé–¢é€£è¨˜äº‹æ¤œå‡º
    ai_articles = []
    ai_keyword_stats = Counter()
    
    for article in articles:
        text_to_check = (article['title'] + ' ' + article['summary'])
        detected_keywords = detect_ai_keywords(text_to_check)
        
        if detected_keywords:
            ai_articles.append({
                **article,
                'ai_keywords': detected_keywords
            })
            
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰çµ±è¨ˆ
            for category, keyword in detected_keywords:
                ai_keyword_stats[keyword] += 1
    
    ai_rate = len(ai_articles) / len(articles) * 100
    print(f"\nğŸ¤– AIé–¢é€£åˆ†æ:")
    print(f"  AIè¨˜äº‹æ•°: {len(ai_articles)}ä»¶")
    print(f"  AIç‡: {ai_rate:.1f}%")
    
    # äººæ°—AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    if ai_keyword_stats:
        print(f"\nğŸ”¥ äººæ°—AIã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Top10:")
        for keyword, count in ai_keyword_stats.most_common(10):
            print(f"  {keyword}: {count}å›")
    
    # AIè¨˜äº‹ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    if ai_articles:
        print(f"\nğŸ“° AIè¨˜äº‹ã‚µãƒ³ãƒ—ãƒ«:")
        for i, article in enumerate(ai_articles[:5], 1):
            keywords = [kw for _, kw in article['ai_keywords'][:3]]
            keywords_str = ', '.join(keywords) if keywords else 'AIé–¢é€£'
            print(f"  {i}. [{keywords_str}] {article['title'][:50]}...")
    
    return {
        'source_type': source_type,
        'total_articles': len(articles),
        'ai_articles': len(ai_articles),
        'ai_rate': ai_rate,
        'top_keywords': dict(ai_keyword_stats.most_common(10)),
        'sources_count': len(source_counts)
    }

def save_analysis_results(results):
    """åˆ†æçµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    analyze_dir = Path("dataproc/analyze")
    analyze_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ†æçµæœãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
    analysis_data = {
        "analysis_date": datetime.now().isoformat(),
        "week_period": f"{(datetime.now() - timedelta(days=6)).strftime('%Y-%m-%d')} to {datetime.now().strftime('%Y-%m-%d')}",
        "sources": [],
        "summary": {}
    }
    
    # å…¨ä½“çµ±è¨ˆè¨ˆç®—
    total_articles = sum(r['total_articles'] for r in results if r)
    total_ai = sum(r['ai_articles'] for r in results if r)
    
    analysis_data["summary"] = {
        "total_articles": total_articles,
        "total_ai_articles": total_ai,
        "overall_ai_rate": total_ai/total_articles*100 if total_articles > 0 else 0,
        "active_sources": len([r for r in results if r])
    }
    
    # ã‚½ãƒ¼ã‚¹åˆ¥ãƒ‡ãƒ¼ã‚¿
    for result in results:
        if result:
            volume_share = result['total_articles'] / total_articles * 100 if total_articles > 0 else 0
            ai_share = result['ai_articles'] / total_ai * 100 if total_ai > 0 else 0
            efficiency_index = ai_share / volume_share if volume_share > 0 else 0
            
            source_data = {
                "source_type": result['source_type'],
                "total_articles": result['total_articles'],
                "ai_articles": result['ai_articles'],
                "ai_rate": result['ai_rate'],
                "volume_share": volume_share,
                "ai_contribution": ai_share,
                "efficiency_index": efficiency_index,
                "top_keywords": result['top_keywords'],
                "sources_count": result['sources_count']
            }
            analysis_data["sources"].append(source_data)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    week_id = datetime.now().strftime('%Y-W%U')
    filename = analyze_dir / f"sources_analysis_{week_id}_{timestamp}.json"
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(analysis_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ åˆ†æçµæœä¿å­˜: {filename}")
    return str(filename)

def compare_sources(results):
    """ã‚½ãƒ¼ã‚¹é–“æ¯”è¼ƒ"""
    print(f"\n{'='*60}")
    print("ğŸ¯ å…¨ã‚½ãƒ¼ã‚¹æ¯”è¼ƒã‚µãƒãƒªãƒ¼")
    print(f"{'='*60}")
    
    # å…¨ä½“çµ±è¨ˆ
    total_articles = sum(r['total_articles'] for r in results if r)
    total_ai = sum(r['ai_articles'] for r in results if r)
    
    print(f"ğŸ“Š å…¨ä½“çµ±è¨ˆ:")
    print(f"  ç·è¨˜äº‹æ•°: {total_articles:,}ä»¶")
    print(f"  AIè¨˜äº‹æ•°: {total_ai:,}ä»¶")
    print(f"  å…¨ä½“AIç‡: {total_ai/total_articles*100:.1f}%")
    
    # ã‚½ãƒ¼ã‚¹åˆ¥æ¯”è¼ƒ
    print(f"\nğŸ“ˆ ã‚½ãƒ¼ã‚¹åˆ¥è©³ç´°:")
    print(f"{'ã‚½ãƒ¼ã‚¹':<12} {'è¨˜äº‹æ•°':<8} {'AIè¨˜äº‹':<8} {'AIç‡':<8} {'åŠ¹ç‡':<8}")
    print("-" * 50)
    
    for result in results:
        if result:
            efficiency = result['ai_articles'] / result['total_articles'] if result['total_articles'] > 0 else 0
            contribution = result['ai_articles'] / total_ai * 100 if total_ai > 0 else 0
            print(f"{result['source_type']:<12} {result['total_articles']:<8,} {result['ai_articles']:<8} {result['ai_rate']:<7.1f}% {contribution:<7.1f}%")
    
    # è²¢çŒ®åº¦åˆ†æ
    print(f"\nğŸ’¡ è²¢çŒ®åº¦åˆ†æ:")
    for result in results:
        if result:
            contribution = result['ai_articles'] / total_ai * 100 if total_ai > 0 else 0
            volume_share = result['total_articles'] / total_articles * 100 if total_articles > 0 else 0
            efficiency_ratio = contribution / volume_share if volume_share > 0 else 0
            
            print(f"  {result['source_type']}:")
            print(f"    è¨˜äº‹é‡ã‚·ã‚§ã‚¢: {volume_share:.1f}%")
            print(f"    AIè¨˜äº‹ã‚·ã‚§ã‚¢: {contribution:.1f}%")
            print(f"    åŠ¹ç‡æŒ‡æ•°: {efficiency_ratio:.2f} {'â­' if efficiency_ratio > 1 else ''}")
    
    # æ¨å¥¨æ§‹æˆ
    print(f"\nğŸ¯ ãƒ‡ãƒ¼ã‚¿åé›†æˆ¦ç•¥:")
    rss_result = next((r for r in results if r and r['source_type'] == 'RSS'), None)
    youtube_result = next((r for r in results if r and r['source_type'] == 'YouTube'), None)
    aiweekly_result = next((r for r in results if r and r['source_type'] == 'AI-Weekly'), None)
    
    if rss_result:
        print(f"  ğŸ“¡ RSS: å¹…åºƒã„ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆ{rss_result['ai_rate']:.1f}% AIç‡ï¼‰")
    if youtube_result:
        print(f"  ğŸ“º YouTube: å‹•ç”»ãƒ»å®Ÿæ¼”ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆ{youtube_result['ai_rate']:.1f}% AIç‡ï¼‰")
    if aiweekly_result:
        print(f"  ğŸ“° AI-Weekly: å³é¸AIæƒ…å ±ï¼ˆ{aiweekly_result['ai_rate']:.1f}% AIç‡ï¼‰")

def main():
    print("ğŸ” å…¨ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ†æã‚·ã‚¹ãƒ†ãƒ ")
    print("="*60)
    
    results = []
    
    # RSSåˆ†æ
    rss_articles, rss_status = load_rss_data()
    print(f"ğŸ“¡ RSSèª­ã¿è¾¼ã¿: {rss_status}")
    if rss_articles:
        rss_result = analyze_source_data(rss_articles, "RSS")
        results.append(rss_result)
    
    # YouTubeåˆ†æ
    youtube_articles, youtube_status = load_youtube_data()
    print(f"ğŸ“º YouTubeèª­ã¿è¾¼ã¿: {youtube_status}")
    if youtube_articles:
        youtube_result = analyze_source_data(youtube_articles, "YouTube")
        results.append(youtube_result)
    
    # AI-Weeklyåˆ†æ
    aiweekly_articles, aiweekly_status = load_aiweekly_data()
    print(f"ğŸ“° AI-Weeklyèª­ã¿è¾¼ã¿: {aiweekly_status}")
    if aiweekly_articles:
        aiweekly_result = analyze_source_data(aiweekly_articles, "AI-Weekly")
        results.append(aiweekly_result)
    
    # å…¨ä½“æ¯”è¼ƒ
    if results:
        compare_sources(results)
        
        # åˆ†æçµæœã‚’ä¿å­˜
        saved_file = save_analysis_results(results)
    
    print(f"\n{'='*60}")
    print("âœ… å…¨ã‚½ãƒ¼ã‚¹åˆ†æå®Œäº†")

if __name__ == "__main__":
    main()